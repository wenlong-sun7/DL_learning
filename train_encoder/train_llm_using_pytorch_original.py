# -*- coding: utf-8 -*-
"""train_LLM_using_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NIkrJuKOQYUdHchwdetMlhaysL6oLTQs
"""

import torch
import torch.nn as nn
import torch.optim as optim
import pprint

# learning note: https://medium.com/@sntaus/building-a-mini-gpt-like-language-model-from-scratch-27257bf5c145

def data_and_vocab():
  training_data = {
        "how are you": "i am fine <end>",
        "who is john": "a nice person <end>",
        "who is nice": "john <end>",
        "where is john": "at home <end>",
        "how is john": "i dont know <end>",
        "who are you": "mini gpt model <end>",
        "where are you": "at home <end>",
        "what is computer science": "learn computers <end>"
  }

  data_words = [k for k, _ in training_data.items()]
  target_words = [v for _,v in training_data.items()]

  vocab_words = list(set([element.lower() for nestedlist in [x.split(" ") for x in data_words] for element in nestedlist] + [element.lower() for nestedlist in [x.split(" ") for x in target_words] for element in nestedlist]))
  vocab_words.remove("<end>")
  vocab_words.append("<end>")
  vocab_words.insert(0, "")

  word_to_ix = {vocab_words[k].lower(): k for k in range(len(vocab_words))}
  ix_to_word = {v: k for k, v in word_to_ix.items()}

  # Return all the necessary data and mappings
  return training_data, data_words, target_words, vocab_words, word_to_ix, ix_to_word

tr_d, data_words, target_words, vocab_words, word_to_ix, ix_to_word = data_and_vocab()

print(word_to_ix)
print(ix_to_word)

# function to convert a sequence of words to a tensor of indices

def pad_tensores(tensor_list):
  tensor_counts = len(tensor_list) if not torch.is_tensor(tensor_list) else tensor_list.shape[0]
  # print(tensor_list, 'here')
  max_len = max([t.shape[0] for t in tensor_list])
  padded_tensors = []
  for tensor in tensor_list:
    res_t = torch.zeros(max_len, *tensor.shape[1:]).type(tensor.dtype).to(tensor.device)
    res_t[:tensor.shape[0]] = tensor
    padded_tensors.append(res_t)

  res = torch.cat(padded_tensors)
  f_dim = len(tensor_list)
  s_dim = max_len
  return res.reshape(f_dim, s_dim, *res.shape[1:])

def words_to_tensor(seq_batch, device=None):
  index_batch = []

  for seq in seq_batch:
    words_list = seq.lower().split(" ")
    index_list = [word_to_ix[word.lower()] for word in words_list if word in word_to_ix]
    t = torch.tensor(index_list, dtype=torch.long)
    if device is not None:
      t = t.to(device)
    index_batch.append(t)
  # print(f"intermidate index_batch: {index_batch}, and length {len(index_batch)}")
  return pad_tensores(index_batch)


def tensor_to_words(tensor):
  id_batch = tensor.cpu().numpy().tolist()
  res = []
  for indices in id_batch:
    words = []
    for ix in indices:
      words.append(ix_to_word[ix].lower())
      if ix==word_to_ix["<end>"]:
        break
    res.append(" ".join(words))
  return res


sentences = ["how are you I am fine", "who is john a nice person"]

t = words_to_tensor(sentences)
print(f"after transform tensor: {t}")

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Linear projections for queries, keys, values
        self.linear_q = nn.Linear(embed_dim, embed_dim)
        self.linear_k = nn.Linear(embed_dim, embed_dim)
        self.linear_v = nn.Linear(embed_dim, embed_dim)

        # Output projection
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        # Compute scaled dot-product
        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.head_dim)

        # Optional masking (e.g., padding or causal)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        return attn @ V

    def forward(self, x, mask=None):
        B, T, C = x.shape
        # Project and split into heads
        Q = self.linear_q(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.linear_k(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.linear_v(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)

        # Attention per head
        attn_out = self.scaled_dot_product_attention(Q, K, V, mask)

        # Concatenate heads
        attn_out = attn_out.transpose(1, 2).contiguous().view(B, T, C)
        return self.out_proj(attn_out)

class SelfAttention(nn.Module):
  def __init__(self, embed_size, head_count):
    super(SelfAttention, self).__init__()
    self.embed_size = embed_size
    self.head_count = head_count

    # create linear
    self.query_layers = nn.ModuleList([nn.Linear(embed_size, embed_size, bias=False) for _ in range(head_count)])
    self.key_layers = nn.ModuleList([nn.Linear(embed_size, embed_size, bias=False) for _ in range(head_count)])
    self.value_layers = nn.ModuleList([nn.Linear(embed_size, embed_size, bias=False) for _ in range(head_count)])
    self.fc_out = nn.Linear(head_count * embed_size, embed_size)  # Final linear layer to combine head outputs

  def forward(self, embeddings, mask=None):
    batch_size, token_count = embeddings.shape[:2]

    # print(f"attention embeddings shape: {embeddings.shape}")

    qkvs = torch.zeros(self.head_count, 3, batch_size, token_count, self.embed_size).to(embeddings.device)


    # loop through over hads ad compute query
    for head_idx in range(self.head_count):
      qkvs[head_idx,0] = self.query_layers[head_idx](embeddings)
      qkvs[head_idx, 1] = self.key_layers[head_idx](embeddings)
      qkvs[head_idx, 2] = self.value_layers[head_idx](embeddings)

    # compute energy term for each head and pairs of tokens
    energy = torch.zeros(self.head_count, batch_size, token_count, token_count).to(embeddings.device)

    mask = torch.triu(torch.ones((token_count, token_count)), diagonal=1).bool().to(embeddings.device)

    for h in range(self.head_count):
      for b in range(batch_size):
        for i in range(token_count):
          for j in range(token_count):
            energy[h,b,i,j] = torch.dot(qkvs[h,0,b,i],qkvs[h,1,b,j])
        energy[h,b] = energy[h,b].masked_fill(mask, float('-inf'))

    # compute attention socres

    attention = torch.nn.functional.softmax(energy, dim=3)

    # compute output
    out = torch.zeros(batch_size, token_count, self.head_count,self.embed_size).to(embeddings.device)

    # loop over heads and compute query
    for h in range(self.head_count):
      for b in range(batch_size):
        for i in range(token_count):
          out[b,i,h] += (attention[h,b,i,j]*qkvs[h,2,b,j])

    #
    out = out.reshape(batch_size, token_count, self.head_count * self.embed_size)
    out = self.fc_out(out)
    return out


  def masked_attetion(self, energy):
    max_token_count, embed_size, _ = energy.size()
    mask = torch.triu(torch.ones((max_token_count, max_token_count)), diagonal=1)*float('-inf')
    mask = mask.unsqueeze(0).unsqueeze(0)
    mask = mask.expand(batch_size, embed_size, -1, -1)
    masked_scores = energy + mask.to(energy.device)
    return masked_scores.to(energy.device)



class TransformerBlock(nn.Module):
  def __init__(self, embed_size, head_count):
    super(TransformerBlock, self).__init__()
    self.attention = MultiHeadSelfAttention(embed_size, head_count)

    self.norm1 = nn.LayerNorm(embed_size)
    self.norm2 = nn.LayerNorm(embed_size)

    # feed-forward
    self.feed_forward = nn.Sequential(
        nn.Linear(embed_size, embed_size),
        nn.ReLU(),
        nn.Linear(embed_size, embed_size)
    )


  def forward(self, x):
    attention = self.attention(x)
    x = self.norm1(attention + x)
    forward = attention + self.feed_forward(x)
    out = self.norm2(forward)
    return out


class Transformer(nn.Module):
  def __init__(self, vocab_size, embed_size, num_layers, head_counts):
    super(Transformer, self).__init__()
    self.embed_size = embed_size
    self.vocab_size = vocab_size
    self.word_embedding = nn.Embedding(vocab_size, embed_size)
    # self.position_embedding = nn.Embedding(vocab_size, embed_size)

    #list of tranformer blocks
    self.layers = nn.ModuleList([TransformerBlock(embed_size, head_counts) for _ in range(num_layers)])

    self.fc_out = nn.Linear(embed_size, vocab_size)

  def forward(self,input_tokens, mask=None):
    # print(f"transformer input tokens: {input_tokens}, shape is {input_tokens.shape}")
    batch_size, token_count = input_tokens.shape[:2]

    out = self.word_embedding(input_tokens)

    # compute position encodings
    positions = torch.arange(0, token_count).expand(batch_size, token_count).to(input_tokens.device)

    position_embedding = self.position_embedding(positions, self.embed_size)

    out = out + position_embedding.reshape(out.shape)

    for layer in self.layers:
      out = layer(out)

    # out = self.fc_out(out[:,-1,:].reshape(batch_size,self.embed_size)).reshape(batch_size, self.vocab_size)
    out = self.fc_out(out[:,-1,:].reshape(batch_size,self.embed_size))
    return out
    # return torch.nn.functional.softmax(out, dim=1)

  def position_embedding(self, positions, embed_size):
    # Compute position encoding for each position and dimension
    angle_rads = self.get_angles(
        positions.unsqueeze(2).float(),
        torch.arange(embed_size)[None, None, :].float().to(positions.device),
        embed_size
    )
    sines = torch.sin(angle_rads[:, :, 0::2])  # Compute sine of angle for even dimensions
    cosines = torch.cos(angle_rads[:, :, 1::2])  # Compute cosine of angle for odd dimensions
    pos_encoding = torch.cat([sines, cosines], dim=-1)  # Concatenate sine and cosine values
    pos_encoding = pos_encoding.squeeze(1)
    return pos_encoding.to(positions.device)

  def get_angles(self, pos, i, embed_size):
    # Compute angle rate for each position and dimension
    angle_rates = 1 / torch.pow(10000, (2 * (i//2)) / embed_size)
    return pos * angle_rates

def train_recursive(model, data, targets, optimizer, criterion):
  model.train()
  optimizer.zero_grad()
  total_loss = 0

  batch_size, token_count, token_count_out = data.shape[0], data.shape[1], targets.shape[1]
  # print(f"batch size: {batch_size},token_count: {token_count}, token_count_out: {token_count_out}")
  for b in range(batch_size):
    # print(f"batch size: {batch_size}")
    end_encountered = False
    cur_count = 0
    # print(f"data is here: {data[b]}")
    while not end_encountered:
      target_vector = torch.zeros(model.vocab_size).to(data.device)
      if cur_count != token_count_out:
        expected_next_token_idx = targets[b, cur_count]
        target_vector[expected_next_token_idx] = 1

      if cur_count>0:
        model_input = data[b].reshape(token_count).to(data.device)
        part_of_output = targets[b, :cur_count].to(data.device)

        # print(f"part of output: {part_of_output}, model_input: {model_input}")
        model_input = torch.cat((model_input, part_of_output))
      else:
        model_input = data[b]

      out = model(model_input.reshape(1, token_count+ cur_count))
      loss = criterion(out, target_vector.reshape(out.shape))

      total_loss += loss.item()
      cur_count += 1

      if cur_count>token_count_out:
        end_encountered = True
  loss.backward()
  optimizer.step()
  return total_loss / batch_size




def infer_recursive(model, input_vectors, max_output_token_count=10):
  model.eval()
  outputs = []

  for i in range(input_vectors.shape[0]):
    print(f"Inferring {i}")
    input_vector = input_vectors[i].reshape(1,input_vectors.shape[1])
    predicted_sequence= []
    wc = 0

    with torch.no_grad():
      while True:
        output = model(input_vector)
        predicted_token = output[0,:].argmax().item()
        predicted_sequence.append(predicted_token)

        if predicted_token == word_to_ix["<end>"] or wc>max_output_token_count:
          break
        new_token_tensor = torch.tensor([predicted_token]).to(input_vector.device)
        new_token_tensor = new_token_tensor.unsqueeze(0)
        input_vector = torch.cat([input_vector, new_token_tensor], dim=1)
        wc+=1

    outputs.append(torch.tensor(predicted_sequence))
  outputs = pad_tensores(outputs)
  return outputs



# def example_train():
#   vocab_size= len(word_to_ix)
#   embed_size = 64
#   num_layers =2
#   heads = 2

#   # create model, optimizer, loos function

#   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#   model = Transformer(vocab_size, embed_size, num_layers, heads).to(device)
#   optimizer= optim.Adam(model.parameters(), lr=0.001)
#   criterion = nn.CrossEntropyLoss()

#   #convert training data to tensors
#   data = words_to_tensor(data_words, device=device)
#   targets = words_to_tensor(target_words, device=device)

#   for epoch in range(2):
#     loss = train_recursive(model, data, targets, optimizer, criterion)
#     print(f"Epoch {epoch+1}, Loss: {loss}")

#   #perform inferrence on training data
#   input_vector = words_to_tensor(data_words, device=device)
#   predicted_vector = infer_recursive(model, input_vector)
#   predicted_words = tensor_to_words(predicted_vector)

#   #
#   pprint.pprint(list(zip(data_words, target_words, predicted_words)))




training_data, data_words, target_words, vocabulary_words, word_to_ix, ix_to_word = data_and_vocab()
# # Run the example training and inference function
# example_train()

vocab_size= len(word_to_ix)
embed_size = 256
num_layers =4
heads = 2

# create model, optimizer, loos function

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"device {device}")
model = Transformer(vocab_size, embed_size, num_layers, heads).to(device)
print(f"model created")
optimizer= optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

#convert training data to tensors
data = words_to_tensor(data_words, device=device)
targets = words_to_tensor(target_words, device=device)

for epoch in range(10):
  print(f"epoch {epoch}")
  loss = train_recursive(model, data, targets, optimizer, criterion)
  print(f"Epoch {epoch+1}, Loss: {loss}")

#perform inferrence on training data
input_vector = words_to_tensor(data_words, device=device)
predicted_vector = infer_recursive(model, input_vector)
predicted_words = tensor_to_words(predicted_vector)

#
# Print training data and model output
print("\n\n\n")
print("Training Data:")
pprint.pprint(training_data)
print("\n\n")
print("Model Inference:")
result_data = {data_words[k]: predicted_words[k] for k in range(len(predicted_words))}
pprint.pprint(result_data)

# prompt: print model's embedding matrix

model.word_embedding.weight

# prompt: print model's position embedding values

model.position_embedding(torch.arange(0, model.vocab_size).unsqueeze(0).to(device), model.embed_size).shape
model.position_embedding(torch.arange(0, model.vocab_size).unsqueeze(0).to(device), model.embed_size)

